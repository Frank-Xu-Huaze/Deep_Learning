{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.datasets as dsets\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import save_image\n",
    "from torchvision.datasets import MNIST\n",
    "from model_28 import Generator, Discriminator\n",
    "from IPython.display import clear_output\n",
    "import numpy as np\n",
    "import datetime\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "img_transform = transforms.Compose([\n",
    "    transforms.Resize(28),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "dataset = MNIST(root='./data/MNIST', download=True, train=True, transform=img_transform)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cuda(data):\n",
    "    if torch.cuda.is_available():\n",
    "        return data.cuda()\n",
    "    else:\n",
    "        return data\n",
    "\n",
    "def denorm(x):\n",
    "    out = (x + 1) / 2\n",
    "    return out.clamp_(0, 1)\n",
    "    \n",
    "def train(steps = 1000000, batch_size = 64, z_dim = 100):\n",
    "    # Initialize model\n",
    "    G = cuda(Generator(batch_size))\n",
    "    D = cuda(Discriminator(batch_size))\n",
    "    \n",
    "    # Make directory for samples and models\n",
    "    cwd = os.getcwd()\n",
    "    if not os.path.exists(cwd+'/samples'):\n",
    "        os.makedirs(cwd+'/samples')\n",
    "    #if not os.path.exists(cwd+'/models'):\n",
    "        #os.makedirs(cwd+'/models')\n",
    "\n",
    "    # Initialize optimizer with filter, lr and coefficients\n",
    "    g_optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, G.parameters()), 0.0001, [0.0,0.9])\n",
    "    d_optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, D.parameters()), 0.0004, [0.0,0.9])\n",
    "    \n",
    "    # Load data\n",
    "    Iter = iter(dataloader)\n",
    "    \n",
    "    # Fix a random latent input for samples\n",
    "    fixed_z = cuda(torch.randn(batch_size, z_dim))\n",
    "    \n",
    "    # Start timer\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for step in range(steps):\n",
    "        # ================== Train D ================== #\n",
    "        D.train(); G.train()\n",
    "        try:\n",
    "            real_images,_ = next(Iter)\n",
    "        except:\n",
    "            Iter = iter(dataloader)\n",
    "            real_images,_ = next(Iter)\n",
    "        \n",
    "        # Compute loss with real images\n",
    "        d_out_real,_ = D(cuda(real_images))\n",
    "        d_loss_real = - torch.mean(d_out_real)\n",
    "        \n",
    "        # Compute loss with fake images\n",
    "        z = cuda(torch.randn(batch_size, z_dim))\n",
    "        fake_images,_ = G(z)\n",
    "        d_out_fake,_ = D(fake_images)\n",
    "        d_loss_fake = d_out_fake.mean()\n",
    "        \n",
    "        # Backward + Optimize\n",
    "        d_loss = d_loss_real + d_loss_fake\n",
    "        d_optimizer.zero_grad(); g_optimizer.zero_grad()\n",
    "        d_loss.backward()\n",
    "        d_optimizer.step()\n",
    "        \n",
    "        # ================== Train G ================== #\n",
    "        # Create random noise\n",
    "        z = cuda(torch.randn(batch_size, z_dim))\n",
    "        fake_images,_ = G(z)\n",
    "        g_out_fake,_ = D(fake_images)\n",
    "        g_loss_fake = - g_out_fake.mean()\n",
    "        d_optimizer.zero_grad(); g_optimizer.zero_grad()\n",
    "        g_loss_fake.backward()\n",
    "        g_optimizer.step()\n",
    "        \n",
    "        # Print out log info\n",
    "        if (step + 1) % 10 == 0:\n",
    "            elapsed = time.time() - start_time\n",
    "            expect = elapsed/(step + 1)*(steps-step-1)\n",
    "            elapsed = str(datetime.timedelta(seconds=elapsed))\n",
    "            expect = str(datetime.timedelta(seconds=expect))\n",
    "            clear_output(wait=True)\n",
    "            print(\"Elapsed [{}], Expect [{}], step [{}/{}], D_real_loss: {:.4f}, \"\n",
    "                  \" ave_generator_gamma: {:.4f}\".\n",
    "                  format(elapsed,expect,step + 1,steps,d_loss_real.item(),G.attn.gamma.mean().item()))\n",
    "        \n",
    "        # Sample images\n",
    "        if (step + 1) % (100) == 0:\n",
    "            fake_images,_= G(fixed_z)\n",
    "            save_image(denorm(fake_images), os.path.join('./samples', '{}_fake.png'.format(step + 1)))\n",
    "        \n",
    "        # Save models\n",
    "        #if (step+1) % (100) == 0:\n",
    "            #torch.save(G.state_dict(),os.path.join('./models', '{}_G.pth'.format(step + 1)))\n",
    "            #torch.save(D.state_dict(),os.path.join('./models', '{}_D.pth'.format(step + 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed [0:01:58.173807], Expect [16:53:16.159895], step [1940/1000000], D_real_loss: -56151.2891,  ave_generator_gamma: 0.0351\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-2da0ffaf5447>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-26-2e04cc1ed143>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(steps, batch_size, z_dim)\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0md_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0md_loss_real\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0md_loss_fake\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0md_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mg_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m         \u001b[0md_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m         \u001b[0md_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    193\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m         \"\"\"\n\u001b[0;32m--> 195\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "pytorch-gpu.1-4.m48",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-4:m48"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
