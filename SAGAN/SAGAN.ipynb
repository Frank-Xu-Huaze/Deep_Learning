{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import datetime\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.datasets as dsets\n",
    "from torchvision import transforms\n",
    "from torch.autograd import Variable\n",
    "from torchvision.utils import save_image\n",
    "from model import Generator, Discriminator\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensor2var(x, grad=False):\n",
    "    if torch.cuda.is_available():\n",
    "        x = x.cuda()\n",
    "    return Variable(x, requires_grad=grad)\n",
    "\n",
    "def var2tensor(x):\n",
    "    return x.data.cpu()\n",
    "\n",
    "def var2numpy(x):\n",
    "    return x.data.cpu().numpy()\n",
    "\n",
    "def denorm(x):\n",
    "    out = (x + 1) / 2\n",
    "    return out.clamp_(0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "imsize = 64\n",
    "g_conv_dim = 64\n",
    "d_conv_dim = 64\n",
    "z_dim = 100\n",
    "beta1 = 0.0\n",
    "beta2 = 0.9\n",
    "total_step = 1000000\n",
    "\n",
    "options = []\n",
    "options.append(transforms.CenterCrop(160))\n",
    "options.append(transforms.Resize((imsize,imsize)))\n",
    "options.append(transforms.ToTensor())\n",
    "options.append(transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)))\n",
    "dataset = dsets.ImageFolder(os.getcwd(), transform=transforms.Compose(options))\n",
    "loader = torch.utils.data.DataLoader(dataset=dataset, batch_size=batch_size,shuffle=True,\n",
    "                                     num_workers=2,drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    # Initialize model\n",
    "    G = Generator(batch_size, imsize, z_dim, g_conv_dim).cuda()\n",
    "    D = Discriminator(batch_size, imsize, d_conv_dim).cuda()\n",
    "    \n",
    "    # Initialize optimizer with filter, lr and coefficients\n",
    "    g_optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, G.parameters()), 0.0001, [beta1, beta2])\n",
    "    d_optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, D.parameters()), 0.0004, [beta1, beta2])\n",
    "    data_iter = iter(loader)\n",
    "    step_per_epoch = len(loader)\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Fix a random latent input for Generator\n",
    "    fixed_z = tensor2var(torch.randn(batch_size, z_dim))\n",
    "    \n",
    "    # Training, total_step as the number of total batches trained \n",
    "    for step in range(total_step):\n",
    "        # ================== Train D ================== #\n",
    "        D.train();G.train()\n",
    "        try:\n",
    "            real_images, _ = next(data_iter)\n",
    "        except:\n",
    "            data_iter = iter(loader)\n",
    "            real_images, _ = next(data_iter)\n",
    "        \n",
    "        # Compute loss with real images\n",
    "        # dr1, dr2, df1, df2, gf1, gf2 are attention scores\n",
    "        real_images = tensor2var(real_images)\n",
    "        d_out_real,dr1,dr2 = D(real_images)\n",
    "        d_loss_real = - torch.mean(d_out_real)\n",
    "        \n",
    "        # apply Gumbel Softmax\n",
    "        z = tensor2var(torch.randn(real_images.size(0), z_dim))\n",
    "        fake_images,gf1,gf2 = G(z)\n",
    "        d_out_fake,df1,df2 = D(fake_images)\n",
    "        d_loss_fake = d_out_fake.mean()\n",
    "        \n",
    "        # Backward + Optimize\n",
    "        d_loss = d_loss_real + d_loss_fake\n",
    "        d_optimizer.zero_grad(); g_optimizer.zero_grad()\n",
    "        d_loss.backward()\n",
    "        d_optimizer.step()\n",
    "        \n",
    "        # Compute gradient penalty\n",
    "        alpha = torch.rand(real_images.size(0), 1, 1, 1).cuda().expand_as(real_images)\n",
    "        interpolated = Variable(alpha * real_images.data + (1 - alpha) * fake_images.data, requires_grad=True)\n",
    "        out,_,_ = D(interpolated)\n",
    "\n",
    "        grad = torch.autograd.grad(outputs=out,\n",
    "                                    inputs=interpolated,\n",
    "                                    grad_outputs=torch.ones(out.size()).cuda(),\n",
    "                                    retain_graph=True,\n",
    "                                    create_graph=True,\n",
    "                                    only_inputs=True)[0]\n",
    "\n",
    "        grad = grad.view(grad.size(0), -1)\n",
    "        grad_l2norm = torch.sqrt(torch.sum(grad ** 2, dim=1))\n",
    "        d_loss_gp = torch.mean((grad_l2norm - 1) ** 2)\n",
    "\n",
    "        # Backward + Optimize\n",
    "        d_loss = 10 * d_loss_gp\n",
    "        d_optimizer.zero_grad(); g_optimizer.zero_grad()\n",
    "        d_loss.backward()\n",
    "        d_optimizer.step()\n",
    "        \n",
    "        # ================== Train G and gumbel ================== #\n",
    "        # Create random noise\n",
    "        z = tensor2var(torch.randn(real_images.size(0), z_dim))\n",
    "        fake_images,_,_ = G(z)\n",
    "\n",
    "        # Compute loss with fake images\n",
    "        g_out_fake,_,_ = D(fake_images)  # batch x n\n",
    "        g_loss_fake = - g_out_fake.mean()\n",
    "        d_optimizer.zero_grad(); g_optimizer.zero_grad()\n",
    "        g_loss_fake.backward()\n",
    "        g_optimizer.step()\n",
    "\n",
    "\n",
    "        # Print out log info\n",
    "        if (step + 1) % 10 == 0:\n",
    "            elapsed = time.time() - start_time\n",
    "            elapsed = str(datetime.timedelta(seconds=elapsed))\n",
    "            print(\"Elapsed [{}], G_step [{}/{}], D_step[{}/{}], d_out_real: {:.4f}, \"\n",
    "                  \" ave_gamma_l3: {:.4f}, ave_gamma_l4: {:.4f}\".\n",
    "                  format(elapsed, step + 1, total_step, (step + 1),\n",
    "                         total_step , d_loss_real.item(),\n",
    "                         G.attn1.gamma.mean().item(), G.attn2.gamma.mean().item()))\n",
    "\n",
    "        # Sample images\n",
    "        if (step + 1) % 100 == 0:\n",
    "            fake_images,_,_= G(fixed_z)\n",
    "            save_image(denorm(fake_images.data),\n",
    "                        os.path.join('./samples', '{}_fake.png'.format(step + 1)))\n",
    "        \n",
    "        # Save models\n",
    "        if (step+1) % 100==0:\n",
    "            torch.save(G.state_dict(),\n",
    "                        os.path.join('./models', '{}_G.pth'.format(step + 1)))\n",
    "            torch.save(D.state_dict(),\n",
    "                        os.path.join('./models', '{}_D.pth'.format(step + 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./mnist_data/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee115d497ae54252a371b07ee40c5c3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./mnist_data/MNIST/raw/train-images-idx3-ubyte.gz to ./mnist_data/MNIST/raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./mnist_data/MNIST/raw/train-labels-idx1-ubyte.gz\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba02f2558208406ebcc21c802cf56bf8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./mnist_data/MNIST/raw/train-labels-idx1-ubyte.gz to ./mnist_data/MNIST/raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./mnist_data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d12a9e877e9c463a82a43c0f5707b06e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./mnist_data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./mnist_data/MNIST/raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./mnist_data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53388762df1b45fdbf6d7db3d6cd300d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./mnist_data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./mnist_data/MNIST/raw\n",
      "Processing...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.datasets as dsets\n",
    "from torchvision import transforms\n",
    "batch_size = 64\n",
    "\n",
    "options = []\n",
    "options.append(transforms.ToTensor())\n",
    "options.append(transforms.Normalize((0.1307,), (0.3081,)))\n",
    "mnist = dsets.MNIST(root='./mnist_data', train=True, download=True, transform=transforms.Compose(options))\n",
    "train_loader = torch.utils.data.DataLoader(dataset=mnist, batch_size=batch_size, shuffle=True,\n",
    "                                           num_workers=2, drop_last=True)\n",
    "Iter = iter(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 1, 28, 28])"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x,y =next(Iter)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_dim = 64\n",
    "\n",
    "layer1 = []\n",
    "layer1.append(nn.Conv2d(1, conv_dim, 3, 2, 1)) #(1->64, 28->14)\n",
    "layer1.append(nn.LeakyReLU(0.1))\n",
    "l1 = nn.Sequential(*layer1)\n",
    "\n",
    "\n",
    "layer2 = []\n",
    "curr_dim = conv_dim\n",
    "layer2.append(nn.Conv2d(curr_dim, curr_dim * 2, 3, 2, 1))#(64->128, 14->7)\n",
    "layer2.append(nn.LeakyReLU(0.1))\n",
    "l2 = nn.Sequential(*layer2)\n",
    "\n",
    "\n",
    "layer3 = []\n",
    "curr_dim = curr_dim * 2\n",
    "layer3.append(nn.Conv2d(curr_dim, curr_dim * 2, 3, 2, 1))#(128->256, 7->4)\n",
    "layer3.append(nn.LeakyReLU(0.1))\n",
    "l3 = nn.Sequential(*layer3)\n",
    "\n",
    "curr_dim = curr_dim * 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 256, 4, 4])"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = l1(x)\n",
    "out = l2(out)\n",
    "out = l3(out)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ATTN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "# out.shape = torch.Size([64, 256, 4, 4])\n",
    "m_batchsize,C,width ,height = out.size()\n",
    "\n",
    "# The effect of the 1 X 1 convolution is it just adds non-linearity.\n",
    "query_conv = nn.Conv2d(in_channels = curr_dim , out_channels = curr_dim//8 , kernel_size= 1)\n",
    "key_conv = nn.Conv2d(in_channels = curr_dim , out_channels = curr_dim//8 , kernel_size= 1)\n",
    "value_conv = nn.Conv2d(in_channels = curr_dim , out_channels = curr_dim , kernel_size= 1)\n",
    "softmax = nn.Softmax(dim=-1)\n",
    "gamma = nn.Parameter(torch.zeros(1))\n",
    "\n",
    "proj_query = query_conv(out).view(m_batchsize,-1,width*height).permute(0,2,1) # B * N * C, N = W*H\n",
    "# torch.Size([64, 16, 32])\n",
    "\n",
    "proj_key =  key_conv(out).view(m_batchsize,-1,width*height) # B * C * N, N = W*H\n",
    "# torch.Size([64, 32, 16])\n",
    "\n",
    "energy =  torch.bmm(proj_query,proj_key) # batch matrix-matrix product\n",
    "# torch.Size([64, 16, 16])\n",
    "\n",
    "attention = softmax(energy) # softmax to ensure for the output sums up to 1, as a weight\n",
    "# torch.Size([64, 16, 16])\n",
    "\n",
    "proj_value = value_conv(out).view(m_batchsize,-1,width*height) # B * C * N\n",
    "# torch.Size([64, 256, 16])\n",
    "\n",
    "att = torch.bmm(proj_value,attention.permute(0,2,1)) # batch matrix-matrix product\n",
    "# torch.Size([64, 256, 16])\n",
    "att = att.view(m_batchsize,C,width,height)\n",
    "# torch.Size([64, 256, 4, 4])\n",
    "\n",
    "out1 = gamma*att + out\n",
    "# torch.Size([64, 256, 4, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1000, 3, 64, 64])"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imsize = 64\n",
    "options = []\n",
    "options.append(transforms.CenterCrop(160))\n",
    "options.append(transforms.Resize((imsize,imsize)))\n",
    "options.append(transforms.ToTensor())\n",
    "options.append(transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)))\n",
    "dataset = dsets.ImageFolder(os.getcwd(), transform=transforms.Compose(options))\n",
    "loader = torch.utils.data.DataLoader(dataset=dataset, batch_size=1000,shuffle=True,\n",
    "                                     num_workers=2,drop_last=True)\n",
    "Iter = iter(loader)\n",
    "x,y =next(Iter)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1000, 256, 8, 8])"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_dim = 64\n",
    "\n",
    "layer1 = []\n",
    "layer1.append(nn.Conv2d(3, conv_dim, 4, 2, 1)) #(1->64, 28->14)\n",
    "layer1.append(nn.LeakyReLU(0.1))\n",
    "l1 = nn.Sequential(*layer1)\n",
    "\n",
    "\n",
    "layer2 = []\n",
    "curr_dim = conv_dim\n",
    "layer2.append(nn.Conv2d(curr_dim, curr_dim * 2, 4, 2, 1))#(64->128, 14->7)\n",
    "layer2.append(nn.LeakyReLU(0.1))\n",
    "l2 = nn.Sequential(*layer2)\n",
    "\n",
    "\n",
    "layer3 = []\n",
    "curr_dim = curr_dim * 2\n",
    "layer3.append(nn.Conv2d(curr_dim, curr_dim * 2, 4, 2, 1))#(128->256, 7->4)\n",
    "layer3.append(nn.LeakyReLU(0.1))\n",
    "l3 = nn.Sequential(*layer3)\n",
    "\n",
    "curr_dim = curr_dim * 2 \n",
    "\n",
    "out = l1(x)\n",
    "out = l2(out)\n",
    "out = l3(out)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# out.shape = torch.Size([64, 256, 4, 4])\n",
    "m_batchsize,C,width ,height = out.size()\n",
    "\n",
    "# The effect of the 1 X 1 convolution is it just adds non-linearity.\n",
    "query_conv = nn.Conv2d(in_channels = curr_dim , out_channels = curr_dim//8 , kernel_size= 1)\n",
    "key_conv = nn.Conv2d(in_channels = curr_dim , out_channels = curr_dim//8 , kernel_size= 1)\n",
    "value_conv = nn.Conv2d(in_channels = curr_dim , out_channels = curr_dim , kernel_size= 1)\n",
    "softmax = nn.Softmax(dim=-1)\n",
    "gamma = nn.Parameter(torch.zeros(1))\n",
    "\n",
    "proj_query = query_conv(out).view(m_batchsize,-1,width*height).permute(0,2,1) # B * N * C, N = W*H\n",
    "# torch.Size([64, 16, 32])\n",
    "\n",
    "proj_key =  key_conv(out).view(m_batchsize,-1,width*height) # B * C * N, N = W*H\n",
    "# torch.Size([64, 32, 16])\n",
    "\n",
    "energy =  torch.bmm(proj_query,proj_key) # batch matrix-matrix product\n",
    "# torch.Size([64, 16, 16])\n",
    "\n",
    "attention = softmax(energy) # softmax to ensure for the output sums up to 1, as a weight\n",
    "# torch.Size([64, 16, 16])\n",
    "\n",
    "proj_value = value_conv(out).view(m_batchsize,-1,width*height) # B * C * N\n",
    "# torch.Size([64, 256, 16])\n",
    "\n",
    "att = torch.bmm(proj_value,attention.permute(0,2,1)) # batch matrix-matrix product\n",
    "# torch.Size([64, 256, 16])\n",
    "att = att.view(m_batchsize,C,width,height)\n",
    "# torch.Size([64, 256, 4, 4])\n",
    "\n",
    "out1 = gamma*att + out\n",
    "# torch.Size([64, 256, 4, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<filter at 0x123c5e8d0>"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G = Generator(64, 64, 100, 64)\n",
    "filter(lambda p: p.requires_grad, G.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spectral import SpectralNorm\n",
    "batch_size = 64\n",
    "imsize=28\n",
    "z_dim=100 \n",
    "conv_dim=64\n",
    "\n",
    "# Layer 1 turn 100 dims -> 64 dims, 1 -> 3\n",
    "layer1 = []\n",
    "layer1.append(SpectralNorm(nn.ConvTranspose2d(in_channels = z_dim, out_channels = conv_dim, kernel_size = 4)))\n",
    "layer1.append(nn.BatchNorm2d(conv_dim))\n",
    "layer1.append(nn.ReLU())\n",
    "curr_dim = conv_dim\n",
    "l1 = nn.Sequential(*layer1)\n",
    "        \n",
    "# Layer 2 turn 64 dims -> 32 dims, 3 -> 7\n",
    "layer2 = []\n",
    "layer2.append(SpectralNorm(nn.ConvTranspose2d(in_channels = curr_dim, out_channels = int(curr_dim / 2), \n",
    "                                                      kernel_size = 4, stride = 2, padding = 1)))\n",
    "layer2.append(nn.BatchNorm2d(int(curr_dim / 2)))\n",
    "layer2.append(nn.ReLU())\n",
    "curr_dim = int(curr_dim / 2)\n",
    "l2 = nn.Sequential(*layer2)\n",
    "        \n",
    "# Layer 3 turn 32 dims -> 16 dims, 3 -> 14\n",
    "layer3 = []\n",
    "layer3.append(SpectralNorm(nn.ConvTranspose2d(in_channels = curr_dim, out_channels = int(curr_dim / 2), \n",
    "                                                      kernel_size = 4, stride = 2, padding = 1)))\n",
    "layer3.append(nn.BatchNorm2d(int(curr_dim / 2)))\n",
    "layer3.append(nn.ReLU())\n",
    "curr_dim = int(curr_dim / 2)\n",
    "l3 = nn.Sequential(*layer3)\n",
    "\n",
    "# Layer 5 (Attn) turn 16 dims -> 16 dims\n",
    "# self.attn1 = Self_Attn(curr_dim, 'relu')\n",
    "        \n",
    "# Layer 6 turn 16 dims -> 3 dims, 14 -> 28\n",
    "last = []\n",
    "last.append(nn.ConvTranspose2d(curr_dim, 3, 4, 2, 1))\n",
    "last.append(nn.Tanh())\n",
    "last = nn.Sequential(*last)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 32, 8, 8])"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = tensor2var(torch.randn(batch_size, z_dim))\n",
    "z = z.view(z.size(0), z.size(1), 1, 1) # torch.Size([64, 100, 1, 1])\n",
    "out=l1(z) # torch.Size([64, 64, 3, 3])\n",
    "out=l2(out) # torch.Size([64, 32, 7, 7])\n",
    "#out=l3(out) # torch.Size([64, 16, 14, 14])\n",
    "#out=last(out) # torch.Size([64, 3, 28, 28])\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
